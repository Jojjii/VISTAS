# VISTAS
Vision-Integrated System for Tracking Atmospheric Shifts


Vision-Language Processing Pipeline for Climate Disaster Analysis
Project Overview: Developed a cutting-edge multi-modal pipeline integrating advanced Vision-Language Models (VLMs) to analyze and generate insights from visual data depicting climate-related disasters and their most related causes.

Technologies Used: BLIP, LLaMA-3.2, CLIP, Python, TensorFlow/PyTorch, Natural Language Processing (NLP), Image Processing, Machine Learning (ML).

Key Contributions:

Multi-modal Integration: Engineered a pipeline that seamlessly combines BLIP for image captioning and LLaMA-3.2-11B-Vision for cause generation, enabling the analysis of complex climate events through visual and textual data.

Causal Inference Mechanism: Implemented a robust algorithm to generate contextually relevant causes for climate disasters by synthesizing visual input with descriptive text prompts, demonstrating advanced natural language understanding and generation capabilities.

Semantic Similarity Assessment: Leveraged CLIP to evaluate generated causal hypotheses against the original image context, employing a contrastive learning framework to identify the most plausible explanations based on visual features and textual descriptions.

Impactful Insights Generation: Facilitated actionable insights on climate change by enabling rapid analysis of imagery, supporting decision-making processes in environmental policy and disaster management.

Research and Development: Conducted a comprehensive literature review of state-of-the-art VLMs and integrated the latest advancements in NLP and computer vision to ensure high model performance and relevance in climate studies.

Results Achieved:
Successfully enhanced the interpretability of visual data in the context of climate change, contributing to a deeper understanding of environmental impacts and facilitating discussions around sustainability and conservation efforts.
